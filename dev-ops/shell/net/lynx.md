## lynx 以文本方式下载网页

lynx -dump www.baidu.com > plain.txt

网页中的超链接会以列表的形式整理在Reference条目下

```
...

References
   1. http://www.baidu.com/content-search.xml
   2. http://www.baidu.com/baidu.html?from=noscript
   3. LYNXIMGMAP:http://www.baidu.com/#mp
   4. LYNXIMGMAP:http://www.baidu.com/#mp
   5. http://www.baidu.com/
   6. javascript:;
   7. javascript:;


```

> 查找网页中的无效链接

```
#param check
if [ $# -ne 1 ]; then
   echo -e "$0 Usage URL\n"
fi


echo broken links:

mkdir /tmp/$$.lynx

cd /tmp/$$.lynx

lynx -traversal $1 > /dev/null
count=0

#unique sort special file generated by -traversal and redirect to links.txt
echo "links check:"
sort -u reject.dat > links.txt

while read link
do
    output=`curl -I $link -s | grep "HTTP/.*OK"`;
    if [[ -z $output ]]; then
        echo $link
        let count++
    fi
done < links.txt

[ $count -eq 0 ] && echo No broken links found

```
